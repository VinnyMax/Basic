{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1124fdd3",
   "metadata": {},
   "source": [
    "# PYSPARK - RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d3d3c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install arquivo.whl\n",
    "# pip install C:\\Pasta\\arquivo.whl\n",
    "\n",
    "# Se tiver dependências, você pode indicar uma pasta onde procurar as dependências:\n",
    "# pip install --no-index --find-links c:\\Pasta programa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec9abc32",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7894a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext                                    # Importing SparkContext\n",
    "from pyspark.sql import SparkSession, Window, Row                   # Importing importing methods for creating a cluster\n",
    "from pyspark.sql import functions as F                              # Importing SQL Functions\n",
    "from pyspark.sql.functions import col, isnan, when, count           # Importing relevant dataframe functions\n",
    "from pyspark.sql.functions import *                                 # Importing inbuilt SQL Functions\n",
    "from pyspark.sql.types import *                                     # Importing SQL types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d964a8",
   "metadata": {},
   "source": [
    "## Initializing Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914c2b27",
   "metadata": {},
   "source": [
    "### SparkContext "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb732cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext(master = 'local[2]')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c05f823",
   "metadata": {},
   "source": [
    "### Inspect SparkContext "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0763b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.version #Retrieve SparkContext version\n",
    "sc.pythonVer #Retrieve Python version\n",
    "sc.master #Master URL to connect to\n",
    "str(sc.sparkHome) #Path where Spark is installed on worker nodes\n",
    "str(sc.sparkUser()) #Retrieve name of the Spark User running SparkContext\n",
    "sc.appName #Return application name\n",
    "sc.applicationld #Retrieve application ID\n",
    "sc.defaultParallelism #Return default level of parallelism\n",
    "sc.defaultMinPartitions #Default minimum number of partitions for RDDs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6114c27",
   "metadata": {},
   "source": [
    "### Configuration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e8bfd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "conf = (SparkConf()\n",
    "     .setMaster(\"local\")\n",
    "     .setAppName(\"My app\")\n",
    "     . set   (\"spark. executor.memory\",   \"lg\"))\n",
    "sc = SparkContext(conf = conf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a460f47c",
   "metadata": {},
   "source": [
    "### Using the Shell \n",
    "\n",
    "In the PySpark shell, a special interpreter-aware SparkContext is already created in the variable called sc.\n",
    "Set which master the context connects to with the --master argument, and add Python .zip..egg or.py files to the runtime path by passing a comma-separated list to  --py-files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0f01d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "$ ./bin/spark-shell --master local[2]\n",
    "$ ./bin/pyspark --master local[s] --py-files code.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa0e8c31",
   "metadata": {},
   "source": [
    "## Loading Data \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50263c93",
   "metadata": {},
   "source": [
    "##### Parallelized Collections "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39552816",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sc.parallelize([('a',7),('a',2),('b',2)])\n",
    "rdd2 = sc.parallelize([('a',2),('d',1),('b',1)])\n",
    "rdd3 = sc.parallelize(range(100))\n",
    "rdd = sc.parallelize([(\"a\",[\"x\",\"y\",\"z\"]),\n",
    "               (\"b\" [\"p\",\"r,\"])])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4effe1a5",
   "metadata": {},
   "source": [
    "##### External Data\n",
    "Read either one text file from HDFS, a local file system or any Hadoop-supported file system URI with textFile(), or read in a directory of text files with wholeTextFiles(). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a187b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "textFile = sc.textFile(\"/my/directory/•.txt\")\n",
    "textFile2 = sc.wholeTextFiles(\"/my/directory/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545ad126",
   "metadata": {},
   "source": [
    "### Inferring the Schema Using Reflection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c21c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# Load a text file and convert each line to a Row.\n",
    "lines = sc.textFile(\"examples/src/main/resources/people.txt\")\n",
    "parts = lines.map(lambda l: l.split(\",\"))\n",
    "people = parts.map(lambda p: Row(name=p[0], age=int(p[1])))\n",
    "\n",
    "# Infer the schema, and register the DataFrame as a table.\n",
    "schemaPeople = spark.createDataFrame(people)\n",
    "schemaPeople.createOrReplaceTempView(\"people\")\n",
    "\n",
    "# SQL can be run over DataFrames that have been registered as a table.\n",
    "teenagers = spark.sql(\"SELECT name FROM people WHERE age >= 13 AND age <= 19\")\n",
    "\n",
    "# The results of SQL queries are Dataframe objects.\n",
    "# rdd returns the content as an :class:`pyspark.RDD` of :class:`Row`.\n",
    "teenNames = teenagers.rdd.map(lambda p: \"Name: \" + p.name).collect()\n",
    "for name in teenNames:\n",
    "    print(name)\n",
    "# Name: Justin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f48957",
   "metadata": {},
   "source": [
    "### Programmatically Specifying the Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4f2e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data types\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# Load a text file and convert each line to a Row.\n",
    "lines = sc.textFile(\"examples/src/main/resources/people.txt\")\n",
    "parts = lines.map(lambda l: l.split(\",\"))\n",
    "# Each line is converted to a tuple.\n",
    "people = parts.map(lambda p: (p[0], p[1].strip()))\n",
    "\n",
    "# The schema is encoded in a string.\n",
    "schemaString = \"name age\"\n",
    "\n",
    "fields = [StructField(field_name, StringType(), True) for field_name in schemaString.split()]\n",
    "schema = StructType(fields)\n",
    "\n",
    "# Apply the schema to the RDD.\n",
    "schemaPeople = spark.createDataFrame(people, schema)\n",
    "\n",
    "# Creates a temporary view using the DataFrame\n",
    "schemaPeople.createOrReplaceTempView(\"people\")\n",
    "\n",
    "# SQL can be run over DataFrames that have been registered as a table.\n",
    "results = spark.sql(\"SELECT name FROM people\")\n",
    "\n",
    "results.show()\n",
    "# +-------+\n",
    "# |   name|\n",
    "# +-------+\n",
    "# |Michael|\n",
    "# |   Andy|\n",
    "# | Justin|\n",
    "# +-------+"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "297873c0",
   "metadata": {},
   "source": [
    "## Retrieving RDD Information \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ad5b05",
   "metadata": {},
   "source": [
    "#### Basic Information "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93266aca",
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> rdd.getNumPartitions() #List the number of partitions\n",
    ">>> rdd.count() #Count RDD instances 3\n",
    ">>> rdd.countByKey() #Count RDD instances by key\n",
    "defaultdict(<type 'int'>,{'a':2,'b':1})\n",
    ">>> rdd.countByValue() #Count RDD instances by value\n",
    "defaultdict(<type 'int'>,{('b',2):1,('a',2):1,('a',7):1})\n",
    ">>> rdd.collectAsMap() #Return (key,value) pairs as a dictionary\n",
    "   {'a': 2, 'b': 2}\n",
    ">>> rdd3.sum() #Sum of RDD elements 4950\n",
    ">>> sc.parallelize([]).isEmpty() #Check whether RDD is empty\n",
    "True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83cc208",
   "metadata": {},
   "source": [
    "#### Summary \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac85e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> rdd3.max() #Maximum value of RDD elements \n",
    "99\n",
    ">>> rdd3.min() #Minimum value of RDD elements\n",
    "0\n",
    ">>> rdd3.mean() #Mean value of RDD elements \n",
    "49.5\n",
    ">>> rdd3.stdev() #Standard deviation of RDD elements \n",
    "28.866070047722118\n",
    ">>> rdd3.variance() #Compute variance of RDD elements \n",
    "833.25\n",
    ">>> rdd3.histogram(3) #Compute histogram by bins\n",
    "([0,33,66,99],[33,33,34])\n",
    ">>> rdd3.stats() #Summary statistics (count, mean, stdev, max & min)\n",
    "\n",
    "df.summary().show() #like Pandas Describe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aeb7bef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "578fcb20",
   "metadata": {},
   "source": [
    "## Applying Functions \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30dc1fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply a function to each RFD element\n",
    ">>> rdd.map(lambda x: x+(x[1],x[0])).collect()\n",
    "[('a' ,7,7, 'a'),('a' ,2,2, 'a'), ('b' ,2,2, 'b')]\n",
    "#Apply a function to each RDD element and flatten the result\n",
    ">>> rdd5 = rdd.flatMap(lambda x: x+(x[1],x[0]))\n",
    ">>> rdd5.collect()\n",
    "['a',7 , 7 ,  'a' , 'a' , 2,  2,  'a', 'b', 2 , 2, 'b']\n",
    "#Apply a flatMap function to each (key,value) pair of rdd4 without changing the keys\n",
    ">>> rdds.flatMapValues(lambda x: x).collect()\n",
    "[('a', 'x'), ('a', 'y'), ('a', 'z'),('b', 'p'),('b', 'r')]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bcaf4c0",
   "metadata": {},
   "source": [
    "## Selecting Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b95f2d",
   "metadata": {},
   "source": [
    "#### Getting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895ffe29",
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> rdd.collect() #Return a list with all RDD elements \n",
    "[('a', 7), ('a', 2), ('b', 2)]\n",
    ">>> rdd.take(2) #Take first 2 RDD elements \n",
    "[('a', 7),  ('a', 2)]\n",
    ">>> rdd.first() #Take first RDD element\n",
    "('a', 7)\n",
    ">>> rdd.top(2) #Take top 2 RDD elements \n",
    "[('b', 2), ('a', 7)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d11076e",
   "metadata": {},
   "source": [
    "#### Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e4e482",
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> rdd3.sample(False, 0.15, 81).collect() #Return sampled subset of rdd3\n",
    "     [3,4,27,31,40,41,42,43,60,76,79,80,86,97]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dacc389",
   "metadata": {},
   "source": [
    "#### Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92353cb",
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> rdd.filter(lambda x: \"a\" in x).collect() #Filter the RDD\n",
    "[('a',7),('a',2)]\n",
    ">>> rdd5.distinct().collect() #Return distinct RDD values\n",
    "['a' ,2, 'b',7]\n",
    ">>> rdd.keys().collect() #Return (key,value) RDD's keys\n",
    "['a',  'a',  'b']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ecf82e",
   "metadata": {},
   "source": [
    "## Iterating "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da0d3cb",
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> def g (x): print(x)\n",
    ">>> rdd.foreach(g) #Apply a function to all RDD elements\n",
    "('a', 7)\n",
    "('b', 2)\n",
    "('a', 2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ad5940",
   "metadata": {},
   "source": [
    "## Reshaping Data \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a3aefda",
   "metadata": {},
   "source": [
    "#### Reducing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7cce03",
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> rdd.reduceByKey(lambda x,y : x+y).collect() #Merge the rdd values for each key\n",
    "[('a',9),('b',2)]\n",
    ">>> rdd.reduce(lambda a, b: a+ b) #Merge the rdd values\n",
    "('a', 7, 'a' , 2 , 'b' , 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba3e5bd",
   "metadata": {},
   "source": [
    "#### Grouping by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b2afac8",
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> rdd3.groupBy(lambda x: x % 2) #Return RDD of grouped values\n",
    "          .mapValues(list)\n",
    "          .collect()\n",
    ">>> rdd.groupByKey() #Group rdd by key\n",
    "          .mapValues(list)\n",
    "          .collect() \n",
    "[('a',[7,2]),('b',[2])]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75385c0c",
   "metadata": {},
   "source": [
    "#### Aggregating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30131d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    ">> seqOp = (lambda x,y: (x[0]+y,x[1]+1))\n",
    ">>> combOp = (lambda x,y:(x[0]+y[0],x[1]+y[1]))\n",
    "#Aggregate RDD elements of each partition and then the results\n",
    ">>> rdd3.aggregate((0,0),seqOp,combOp) \n",
    "(4950,100)\n",
    "#Aggregate values of each RDD key\n",
    ">>> rdd.aggregateByKey((0,0),seqop,combop).collect() \n",
    "     [('a',(9,2)), ('b',(2,1))]\n",
    "#Aggregate the elements of each partition, and then the results\n",
    ">>> rdd3.fold(0,add)\n",
    "     4950\n",
    "#Merge the values for each key\n",
    ">>> rdd.foldByKey(0, add).collect()\n",
    "[('a' ,9), ('b' ,2)]\n",
    "#Create tuples of RDD elements by applying a function\n",
    ">>> rdd3.keyBy(lambda x: x+x).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d901630",
   "metadata": {},
   "source": [
    "## Mathematical Operations \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23d1f06",
   "metadata": {},
   "outputs": [],
   "source": [
    ">>>> rdd.subtract(rdd2).collect() #Return each rdd value not contained in rdd2\n",
    "[('b' ,2), ('a' ,7)]\n",
    "#Return each (key,value) pair of rdd2 with no matching key in rdd\n",
    ">>> rdd2.subtractByKey(rdd).collect()\n",
    "[('d', 1)1\n",
    ">>>rdd.cartesian(rdd2).collect() #Return the Cartesian product of rdd and rdd2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f6077e",
   "metadata": {},
   "source": [
    "## Sort "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6d682c",
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> rdd2.sortBy(lambda x: x[1]).collect() #Sort RDD by given function\n",
    "[('d',1),('b',1),('a',2)]\n",
    ">>> rdd2.sortByKey().collect() #Sort (key, value) ROD by key\n",
    "[('a' ,2), ('b' ,1), ('d' ,1)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17377222",
   "metadata": {},
   "source": [
    "## Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543a6d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sc.union([employees, employees2])\n",
    "rdd.collect.foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df74ae5a",
   "metadata": {},
   "source": [
    "## Subtract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413ddd12",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Subtract returns an RDD that contains only the elements that are in the first RDD.\n",
    "data = Array((103,\"Mark Choi\",\"Torrance\",\"CA\"), (104,\"Janet Reyes\",\"Rolling Hills\",\"CA\"),(105,\"Lester Cruz\",\"Van Nuys\",\"CA\"))\n",
    "rdd = sc.parallelize(data)\n",
    "data2 = Array((103,\"Mark Choi\",\"Torrance\",\"CA\"))\n",
    "rdd2 = sc.parallelize(data2)\n",
    "employees = rdd.subtract(rdd2)\n",
    "employees.collect.foreach(println)\n",
    "#(105,LesterCruz,Van Nuys,CA)\n",
    "#(104,JanetReyes,Rolling Hills,CA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb02c645",
   "metadata": {},
   "source": [
    "## PARTITION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6d5f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "meuDataset.getNumPartitions() # resposta >>> 10\n",
    "\n",
    "meuDataset2 = meuDataset.repartition(20) # aumentando para 20 partições\n",
    "meuDataset3 = meuDataset.coalesce(2) # reduzindo para 2 partições\n",
    "\n",
    "#Se você já estiver trabalhando com um dataframe, você pode usar o recurso de \n",
    "#converte-lo primeiro em rdd e depois usar esses comandos supracitados, \n",
    "#conforme mostrado abaixo:\n",
    "meuDataset.rdd.getNumPartitions() // resposta: 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e5740d6",
   "metadata": {},
   "source": [
    "## Repartitioning \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f2b18e",
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> rdd.repartition(4) #New RDD with 4 partitions\n",
    ">>> rdd.coalesce(1) #Decrease the number of partitions in the RDD to 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af55a0f",
   "metadata": {},
   "source": [
    "## Coalesce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb3908f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Coalesce reduces the number of partitions in an RDD. \n",
    "#You might want to use coalesce after performing a filter on a large RDD. \n",
    "#While filtering reduces the amount of data consumed by the new RDD, \n",
    "#it inherits the number of partitions of the original RDD. \n",
    "df.coalesce(1).write.mode(\"append\").parquet(\"/user/hive/warehouse/Mytable\")\n",
    "\n",
    "#Example 8-11. Coalescing a large RDD in the PySpark shell\n",
    "# Wildcard input that may match thousands of files\n",
    ">>> input = sc.textFile(\"s3n://log-files/2014/*.log\")\n",
    ">>> input.getNumPartitions()\n",
    "35154\n",
    "# A filter that excludes almost all data\n",
    ">>> lines = input.filter(lambda line: line.startswith(\"2014-10-17\"))\n",
    ">>> lines.getNumPartitions()\n",
    "35154\n",
    "# We coalesce the lines RDD before caching\n",
    ">>> lines = lines.coalesce(5).cache()\n",
    ">>> lines.getNumPartitions()\n",
    "4\n",
    "# Subsequent analysis can operate on the coalesced RDD...\n",
    ">>> lines.count()\n",
    "Serialization Format\n",
    "#When Spark is transferring data over the network or spilling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff4014d",
   "metadata": {},
   "source": [
    "## Repartition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683fe157",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Repartition can both decrease and increase the number of partitions in an RDD. You would \n",
    "#generally use coalesce when reducing partitions since it’s more efficient than repartition. \n",
    "#Increasing the number of partitions is useful for increasing the degree of parallelism when \n",
    "#writing to HDFS. In the following example, we’re writing six Parquet files to HDFS.\n",
    "df.repartition(6).write.mode(\"append\").parquet(\"/user/hive/warehouse/Mytable\")\n",
    "\n",
    "#Note Coalesce is generally faster than repartition. Repartition will perform a full \n",
    "#shuffle, creating new partitions and equally distributing data across worker nodes. \n",
    "#Coalesce minimizes data movement and avoids a full shuffle by using existing partitions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b91034e",
   "metadata": {},
   "source": [
    "## Collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15671068",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Collect returns the entire dataset as an array to the driver program.\n",
    "myCities = sc.parallelize(List(\"tokyo\",\"new york\",\"paris\",\"san francisco\"))\n",
    "myCities.collect"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e138ea",
   "metadata": {},
   "source": [
    "## Foreach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8535afa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Foreach executes a function on each element of the dataset.\n",
    "myCities = sc.parallelize(List(\"tokyo\",\"new york\",\"paris\",\"san francisco\"))\n",
    "myCities.collect.foreach(println)\n",
    "#tokyo\n",
    "#newyork\n",
    "#paris\n",
    "#sanFrancisco"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b194f4",
   "metadata": {},
   "source": [
    "# Saving "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed7b8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> rdd.saveAsTextFile(\"rdd.txt\")\n",
    ">>> rdd.saveAsHadoopFile(\"hdfs:// namenodehost/parent/child\",\n",
    "               'org.apache.hadoop.mapred.TextOutputFormat')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95534705",
   "metadata": {},
   "source": [
    "## Stopping SparkContext "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037efc3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3df939",
   "metadata": {},
   "source": [
    "## Execution "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d57c86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "$ ./bin/spark-submit examples/src/main/python/pi.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259dafcb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0fbf536",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8290f2cd",
   "metadata": {},
   "source": [
    "# PYSPARK - SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50abdcd6",
   "metadata": {},
   "source": [
    "## Initializing SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d497dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark a SparkSession \\\n",
    "     .builder\\\n",
    "     .appName(\"Python Spark SQL basic example\") \\\n",
    "     .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "     .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62fffbdf",
   "metadata": {},
   "source": [
    "## Creating DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "600a77f5",
   "metadata": {},
   "source": [
    "### Fromm RDDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a1c8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ff7fd2",
   "metadata": {},
   "source": [
    "##### Infer Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f2cf77",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext\n",
    "lines = sc.textFile(''people.txt'')\n",
    "parts = lines.map(lambda l: l.split(\",\"))\n",
    "people = parts.map(lambda p: Row(nameap[0],ageaint(p[l])))\n",
    "peopledf = spark.createDataFrame(people)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2275178e",
   "metadata": {},
   "source": [
    "##### Specify Schema\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd365aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "people = parts.map(lambda p: Row(name=p[0],\n",
    "               age=int(p[1].strip())))\n",
    "schemaString = \"name age\"\n",
    "fields = [StructField(field_name, StringType(), True) for field_name in schemaString.split()]\n",
    "schema = StructType(fields)\n",
    "spark.createDataFrame(people, schema).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec9356c",
   "metadata": {},
   "source": [
    "### From Spark Data Sources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f712b6",
   "metadata": {},
   "source": [
    "##### JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db631837",
   "metadata": {},
   "outputs": [],
   "source": [
    "#EX 1\n",
    "df = spark.read.json(\"customer.json\")\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b74ffc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EX 2\n",
    "df2 = spark.read.load(\"people.json\", format=\"json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab697d2",
   "metadata": {},
   "source": [
    "##### Parquet files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed073e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = spark.read.load(\"users.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b50a77e",
   "metadata": {},
   "source": [
    "##### TXT files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc16daf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df4 = spark.read.text(\"people.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a1a4f98",
   "metadata": {},
   "source": [
    "##### CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e3f734",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the data\n",
    "df = spark.read.csv('/content/car data.csv', header=True, inferSchema=\"true\")\n",
    "\n",
    "# Shape of the dataset\n",
    "print('Shape of the dataset: ', (df.count(), len(df.columns)))\n",
    "\n",
    "# Displaying top n=10 rows\n",
    "df.show(n=10)\n",
    "\n",
    "Shape of the dataset:  (301, 9)\n",
    "+-------------+----+-------------+-------------+----------+---------+-----------+------------+-----+\n",
    "|     Car_Name|Year|Selling_Price|Present_Price|Kms_Driven|Fuel_Type|Seller_Type|Transmission|Owner|\n",
    "+-------------+----+-------------+-------------+----------+---------+-----------+------------+-----+\n",
    "|         ritz|2014|         3.35|         5.59|     27000|   Petrol|     Dealer|      Manual|    0|\n",
    "|          sx4|2013|         4.75|         9.54|     43000|   Diesel|     Dealer|      Manual|    0|\n",
    "|         ciaz|2017|         7.25|         9.85|      6900|   Petrol|     Dealer|      Manual|    0|\n",
    "|      wagon r|2011|         2.85|         4.15|      5200|   Petrol|     Dealer|      Manual|    0|\n",
    "|        swift|2014|          4.6|         6.87|     42450|   Diesel|     Dealer|      Manual|    0|\n",
    "|vitara brezza|2018|         9.25|         9.83|      2071|   Diesel|     Dealer|      Manual|    0|\n",
    "|         ciaz|2015|         6.75|         8.12|     18796|   Petrol|     Dealer|      Manual|    0|\n",
    "|      s cross|2015|          6.5|         8.61|     33429|   Diesel|     Dealer|      Manual|    0|\n",
    "|         ciaz|2016|         8.75|         8.89|     20273|   Diesel|     Dealer|      Manual|    0|\n",
    "|         ciaz|2015|         7.45|         8.92|     42367|   Diesel|     Dealer|      Manual|    0|\n",
    "+-------------+----+-------------+-------------+----------+---------+-----------+------------+-----+\n",
    "only showing top 10 rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9fb8485",
   "metadata": {},
   "source": [
    "##### SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d71044",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.sql(\"SELECT * FROM parquet.`examples/src/main/resources/users.parquet`\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb13631",
   "metadata": {},
   "source": [
    "## Data Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62cec4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternatively, displaying the head of the data\n",
    "df.head(n=5)\n",
    "\n",
    "# Getting a description of the dataset, similar to pandas df.describe()\n",
    "df.describe().show()\n",
    "\n",
    "# Checking for dataframe schema, similar to pandas df.info()\n",
    "df.printSchema()   \n",
    "\n",
    "# Checking type of dataframe\n",
    "print(type(df))\n",
    "\n",
    "# Checking type of column data\n",
    "print(type(df.select('Selling_Price')))\n",
    "\n",
    "# Checking the datatypes of columns:\n",
    "# Displaying dtypes of columns\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d05887",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering out the top 5 rows and displaying them:\n",
    "df1 = df.limit(5)\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98715e5b",
   "metadata": {},
   "source": [
    "## Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab57b165",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filter entries of age, only keep those records of which the values are >24\n",
    "df.filter(df[\"age\"]>24).show()\n",
    "df.filter(df[\"age\"]>24).filter(df['Avg_Salary'] > 500000).show() #more than one filter\n",
    "\n",
    "# Remove any duration of 0\n",
    "departures_df = departures_df.filter(departures_df[3] > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54da3c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering out the top 5 rows and displaying them, as well as displaying the rest of the dataframe.\n",
    "# getting the list of Row objects\n",
    "row_list = df.collect()\n",
    "  \n",
    "# Slicing the Python List\n",
    "part1 = row_list[:5]\n",
    "part2 = row_list[5:]\n",
    "  \n",
    "# Converting the slices to PySpark DataFrames\n",
    "slice1 = spark.createDataFrame(part1)\n",
    "slice2 = spark.createDataFrame(part2)\n",
    "  \n",
    "# Printing the first slice\n",
    "print('First DataFrame')\n",
    "slice1.show()\n",
    "  \n",
    "# Printing the second slice\n",
    "print('Second DataFrame')\n",
    "slice2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441b5e85",
   "metadata": {},
   "source": [
    "## Where"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d1f8773",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.where((df['Avg_Salary'] > 500000) & (df['Number_of_houses'] > 2)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf1f7cf",
   "metadata": {},
   "source": [
    "## Duplicate Values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8af6957",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropDuplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37784fb",
   "metadata": {},
   "source": [
    "## Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb0e7fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b07d56d6",
   "metadata": {},
   "source": [
    "##### Select"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263a1de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(\"firstName\").show() #Show all entries in firstName column\n",
    "df.select(\"firstName\",\"lastName\") \\\n",
    "      .show()\n",
    "df.select(\"firstName\", #Show all entries in firstName, age and type\n",
    "              \"age\",\n",
    "              explode(\"phoneNumber\") \\\n",
    "              .alias(\"contactInfo\")) \\\n",
    "      .select(\"contactInfo.type\",\n",
    "              \"firstName\",\n",
    "              \"age\") \\\n",
    "      .show()\n",
    "df.select(df[\"firstName\"],df[\"age\"]+ 1) #Show all entries in firstName and age, .show() add 1 to the entries of age\n",
    "df.select(df['age'] > 24).show() #Show all entries where age >24\n",
    "\n",
    "# Displaying the values of a particular column\n",
    "df.select('Selling_Price').show()\n",
    "\n",
    "# Displaying multiple columns of the dataset:\n",
    "df.select(['Selling_Price','Present_Price']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7212d209",
   "metadata": {},
   "source": [
    "##### When"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b79dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(\"firstName\", #Show firstName and 0 or 1 depending on age >30\n",
    "               F.when(df.age > 30, 1) \\\n",
    "              .otherwise(0)) \\\n",
    "      .show()\n",
    "df[df.firstName.isin(\"Jane\",\"Boris\")] #Show firstName if in the given options\n",
    ".collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b7d97a",
   "metadata": {},
   "source": [
    "##### ISIN - IS IN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb40d5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# isin passando um argumento\n",
    "df.filter(df.Marca.isin('Chevrolet')).display()\n",
    "\n",
    "# isin passando uma lista\n",
    "lista = ['Chevrolet', 'Bmw', 'Toyota']\n",
    "df.filter(df.Marca.isin(lista)).display()\n",
    "\n",
    "# isin usando uma coluna de um sdf ... neste caso é um semileft join\n",
    "df = base.join(df_marcas, ['Marca'], 'leftsemi')\n",
    "\n",
    "# IS NOT IN\n",
    "df.filter(~df.Marca.isin(lista)).display()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c1b0d9",
   "metadata": {},
   "source": [
    "##### Like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0f8389",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(\"firstName\", #Show firstName, and lastName is TRUE if lastName is like Smith\n",
    "              df.lastName.like(\"Smith\")) \\\n",
    "     .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ff0587",
   "metadata": {},
   "source": [
    "##### Startswith - Endswith "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7b85bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(\"firstName\", #Show firstName, and TRUE if lastName starts with Sm\n",
    "              df.lastName \\\n",
    "                .startswith(\"Sm\")) \\\n",
    "      .show()\n",
    "df.select(df.lastName.endswith(\"th\"))\\ #Show last names ending in th\n",
    "      .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1c5a1a",
   "metadata": {},
   "source": [
    "##### Substring "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d7e3b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(df.firstName.substr(1, 3) \\ #Return substrings of firstName\n",
    "                          .alias(\"name\")) \\\n",
    "        .collect()\n",
    "\n",
    "# Fatiar strings - retorna os 7 caraceteres a partir da posição 0\n",
    "df = df.withColumn('NOME', F.substring(F.col('NOME'), 0, 7)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6656daf9",
   "metadata": {},
   "source": [
    "##### Between "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23749174",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(df.age.between(22, 24)) \\ #Show age: values are TRUE if between 22 and 24\n",
    "          .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808d8793",
   "metadata": {},
   "source": [
    "##### Lpad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad012ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adiciona \"CPF\" a esquerda nas colunas que possuem valores. \n",
    "#Como CPF possue por padrão 11 digitos vamos adicionar mais 4 (15)\n",
    "df = df.withColumn('CPF', F.lpad(F.col('CPF'), 15, \"CPF \")) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5327aa21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f57910b2",
   "metadata": {},
   "source": [
    "## Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60576b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# no replace\n",
    "sampleWithoutKeyConsideration = noDuplicateDf1.sample(withReplacement=False, fraction=0.5, seed=200)\n",
    "\n",
    "# with replace\n",
    "sampleWithoutKeyConsideration1 = noDuplicateDf1.sample(withReplacement=True, fraction=0.5, seed=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f6abd0",
   "metadata": {},
   "source": [
    "## Find Frequent Items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef768113",
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicateDataDf.freqItems(cols=['iv1']).show()\n",
    "duplicateDataDf.freqItems(cols=['iv1','iv2']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235d2f09",
   "metadata": {},
   "source": [
    "## Add, Update & Remove Columns "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab82037",
   "metadata": {},
   "source": [
    "##### Adding Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64647d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn('city',df.address.city) \\\n",
    "            .withColumn('postalCode',df.address.postalCode) \\\n",
    "            .withColumn('state',df.address.state) \\\n",
    "            .withColumn('streetAddress',df.address.streetAddress) \\\n",
    "            .withColumn('telePhoneNumber', explode(df.phoneNumber.number)) \\\n",
    "            .withColumn('telePhoneType', explode(df.phoneNumber.type)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc25e97a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding a new column to the dataset\n",
    "df1 = df.withColumn(\"Car New\", df['Present_Price']*2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41428f5e",
   "metadata": {},
   "source": [
    "##### Updating Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e49f956",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumnRenamed('telePhoneNumber', 'phoneNumber')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f8eaafd",
   "metadata": {},
   "source": [
    "##### Removing Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e43cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EX 1\n",
    "df = df.drop(\"address\", \"phoneNumber\")\n",
    "\n",
    "# EX 2\n",
    "df = df.drop(df.address).drop(df.phoneNumber)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9983a2f4",
   "metadata": {},
   "source": [
    "##### Concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc5cf1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EX 1\n",
    "df.select(concat(df.firstname,df.middlename,df.lastname).alias(\"FullName\"))\n",
    "#SILVIOSANTOSSILVA\n",
    "\n",
    "# EX 2\n",
    "df.select(concat_ws('_',df.firstname,df.middlename,df.lastname).alias(\"FullName\")\n",
    "#SILVIO_SANTOS_SILVA\n",
    "\n",
    "# Ex 3 - Unir as trasncr\n",
    "newDataframe = base.groupBy('filename').agg(F.concat_ws('. ', F.collect_list(base.dialog)).alias('dialog'))\n",
    "\n",
    "#Customer\n",
    "#dtCustomer = dataframe[dataframe['speaker']=='Customer'].groupby([coluna_grouped], as_index=False)[coluna_dialog].agg({coluna_dialog: '. '.join})\n",
    "dtCustomer = base.filter(base['speaker']=='Customer').groupBy('filename').agg(F.concat_ws('. ', F.collect_list(base.dialog)).alias('dialog_Customer'))\n",
    "    \n",
    "#Vendor\n",
    "#dtVendor = dataframe[dataframe['speaker']=='Vendor'].groupby([coluna_grouped], as_index=False)[coluna_dialog].agg({coluna_dialog: '. '.join})\n",
    "dtVendor = base.filter(base['speaker']=='Vendor').groupBy('filename').agg(F.concat_ws('. ', F.collect_list(base.dialog)).alias('dialog_Vendor'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e95f1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtcustomer =  dtcustomer.withColumnRenamed('filename', 'filename_c')\n",
    "dtvendor = dtvendor.withColumnRenamed('filename', 'filename_a')\n",
    "\n",
    "ls_customer = newDataframe.join(dtcustomer, newDataframe['filename']==dtcustomer['filename_c'], 'inner')\n",
    "ls_vendor = newDataframe.join(dtvendor, newDataframe['filename']==dtvendor['filename_a'], 'inner')\n",
    "ls_vendor =  ls_vendor.withColumnRenamed('filename', 'filename_ls_a').withColumnRenamed('dialog', 'dialog_ls_a')\n",
    "newDataframe2 = ls_customer.join(ls_vendor, ls_customer['filename']==ls_vendor['filename_ls_a'], 'inner').select('filename', 'dialog', 'dialog_customer', 'dialog_vendor')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d715e777",
   "metadata": {},
   "source": [
    "## Missing & Replacing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61510317",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.na.fill(50).show() #Replace null values\n",
    "df.na.drop().show() #Return new df omitting rows with null values\n",
    "df.na.drop(subset='country').show()\n",
    "df.fillna('0').show() #Replace with 0\n",
    "df.fillna( { 'country':'USA', 'browser':'Safari' } ).show() #replace null in with something in that column\n",
    "df.na \\ #Return new df replacing one value with another\n",
    "       .replace(10, 20) \\\n",
    "       .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7050e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the thresh value is set to 2, any row containing less than two non-null values will be dropped.\n",
    "missingDf.dropna(how ='all',thresh=2).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3700418f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PySpark doesn't have the sophisticated function like Pandas to check for null values.\n",
    "# But we have used a custom code to check for null values in a dataframe.\n",
    "# Creating a dataframe to check null value counts\n",
    "null_df = df.select([count(when(col(c).contains('None') | \\\n",
    "                            col(c).contains('NULL') | \\\n",
    "                            (col(c) == '') | \\\n",
    "                            col(c).isNull() | \\\n",
    "                            isnan(c), c \n",
    "                           )).alias(c)\n",
    "                    for c in df.columns])\n",
    "\n",
    "# Displaying the null value counts dataframe\n",
    "null_df.show()\n",
    "+--------+----+-------------+-------------+----------+---------+-----------+------------+-----+\n",
    "|Car_Name|Year|Selling_Price|Present_Price|Kms_Driven|Fuel_Type|Seller_Type|Transmission|Owner|\n",
    "+--------+----+-------------+-------------+----------+---------+-----------+------------+-----+\n",
    "|       0|   0|            0|            0|         0|        0|          0|           0|    0|\n",
    "+--------+----+-------------+-------------+----------+---------+-----------+------------+-----+"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "477a6daf",
   "metadata": {},
   "source": [
    "## Round"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15d07a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn('swimmerSpeed',round(df.swimmerSpeed, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1adb264",
   "metadata": {},
   "source": [
    "### Datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8fa08cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting Year to datetime\n",
    "df1 = df.withColumn(\"Year\", F.to_date(F.col(\"Year\").cast(\"string\"), 'yyyy'))\n",
    "\n",
    "# Extracting Year\n",
    "df = df1.withColumn('Year', F.year(F.to_timestamp('Year', 'yyyy')))\n",
    "df.show()\n",
    "\n",
    "# Add a column with the current date time\n",
    "df = df. withColumn('Data Atualizacao', F.date_format(F.current_tmestamp(), 'dd MMM yyyy | HH:mm:ss'))\n",
    "# Data atualizacao\n",
    "# 08 Oct 2022 | 07:41:11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d900043d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9c6986",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1952ece5",
   "metadata": {},
   "source": [
    "## Lit\n",
    "The need to create a new column with a constant value can be very common."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bfcc254",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.withColumn('constant',F.lit('finance'))\n",
    "df.select('Customer_subtype','constant').show()\n",
    "\n",
    "[Out]: Customer_subtype, Constant\n",
    "       Rich, finance\n",
    "       Billion, finance\n",
    "       Million, finance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a0bea3",
   "metadata": {},
   "source": [
    "## GroupBy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "701e5a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupBy(\"age\")\\ #Group by age, count the members in the groups\n",
    "      .count() \\\n",
    "      .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329485bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupBy('Customer_subtype').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cffd0f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we can apply any type of sorting to the final results. \n",
    "#Because we have seven columns in the dataframe—all are\n",
    "#categorical columns except for one (Avg_Salary), we can iterate over each \n",
    "#column and apply aggregation as in the following example:\n",
    "for col in df.columns: \n",
    "    if col !='Avg_Salary':\n",
    "        print(f\" Aggregation for {col}\")\n",
    "            df.groupBy(col).count().orderBy('count',ascending=False).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbfba374",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupBy('Customer_main_type').agg(F.mean('Avg_Salary')).show() # F.mean\n",
    "df.groupBy('Customer_main_type').agg(F.max('Avg_Salary')).show() #F.max\n",
    "df.groupBy('Customer_main_type').agg(F.min('Avg_Salary')).show()\n",
    "df.groupBy('Customer_main_type').agg(F.sum('Avg_Salary')).show()\n",
    "df.sort(\"Avg_Salary\", ascending=False).show()\n",
    "\n",
    "df.groupBy('Customer_subtype').agg(F.avg('Avg_Salary').alias('mean_salary'))\\\n",
    "    .orderBy('mean_salary',ascending=False).show(50,False)\n",
    "\n",
    "df.groupBy('Customer_subtype').agg(F.max('Avg_Salary')\\\n",
    "    .alias('max_salary')).orderBy('max_salary',ascending=False).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb393ad6",
   "metadata": {},
   "source": [
    "## Collect\n",
    "Collect list provides all the values in the original order of occurrence \n",
    "(they can be reversed as well), and collect set provides only the unique values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fbd4a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(\"Customer_subtype\").agg(F.collect_list(\"Number_of_houses\")).show()\n",
    "[Out]: Customer, [2,1,2,2,2,1,2]\n",
    "df.groupby(\"Customer_subtype\").agg(F.collect_set(\"Number_of_houses\")).show()\n",
    "[Out]: Customer, [1,2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de58edd2",
   "metadata": {},
   "source": [
    "## Sort "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffba22b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "peopledf.sort(peopledf.age.desc()).collect()\n",
    "df.sort(\"age\", ascending=False).collect()\n",
    "df.orderBy([\"age\",\"city\"],ascending=[0,1]).collect()\n",
    "#Sorting on Two Columns in Different Order\n",
    "swimmerDfSorted3 = swimmerDf.orderBy(\"Occupation\",\"swimTimeInSecond\", ascending=[False,True])\n",
    "\n",
    "#Performing a Partition-Wise Sort (SinglePartition Case)\n",
    "swimmerDf.rdd.getNumPartitions()\n",
    "sortedPartitons = swimmerDf.sortWithinPartitions(\"Occupation\",\"swimTimeInSecond\", ascending=[False,True])\n",
    "\n",
    "swimmerDf1 = swimmerDf.repartition(2)\n",
    "swimmerDf1.rdd.glom().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c68c0d",
   "metadata": {},
   "source": [
    "## Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1462e9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#. PySpark offers a very convenient way to merge and pivot your dataframe values\n",
    "#The idea is to combine this dataframe with the original dataframe, so as to have these region \n",
    "#codes as part of the original dataframe, as a column.\n",
    "region_data = spark.createDataFrame([('Family with grownups','PN'),\n",
    "                                     ('Driven Growers','GJ'),\n",
    "    ('Conservative families','DD'),\n",
    "    ('Cruising Seniors','DL'),\n",
    "    ('Average Family ','MN'),\n",
    "    ('Living well','KA'),\n",
    "    ('Successful hedonists','JH'),\n",
    "    ('Retired and Religious','AX'),\n",
    "    ('Career Loners','HY'),('Farmers','JH')], \n",
    "    schema=StructType().add(\"Customer_main_type\",\"string\").add(\"Region Code\",\"string\")\n",
    "                                    \n",
    "new_df=df.join(region_data,on='Customer_main_type')\n",
    "new_df.groupby(\"Region Code\").count().show()\n",
    "                                    \n",
    "                                    \n",
    "# Two DataFrames\n",
    "innerDf = studentsDf.join(subjectsDf, studentsDf.studentid == subjectsDf.studentid, how= \"inner\")\n",
    "leftOuterDf = studentsDf.join(subjectsDf, studentsDf.studentid == subjectsDf.studentid, how= \"left\")\n",
    "rightOuterDf = studentsDf.join(subjectsDf, studentsDf.studentid == subjectsDf.studentid, how= \"right\")\n",
    "outerDf = studentsDf.join(subjectsDf, studentsDf.studentid == subjectsDf.studentid, how= \"outer\")\n",
    "                                    \n",
    "# FAZENDO O JOIN ENTRE DOIS DATAFRAMES\n",
    "df_final = df_2.join(df, df_2['cd_customer'] == df['cd_customer'], \"inner\").filter(F.col('soma_data') >= F.current_date())\n",
    "                                    \n",
    "                                    \n",
    "# isin usando uma coluna de um sdf ... neste caso é um semileft join\n",
    "df = base.join(df_marcas_premium, ['Marca'], 'leftsemi')                                    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415f9c6d",
   "metadata": {},
   "source": [
    "## Pivoting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2bded8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupBy('Customer_main_type').pivot('Avg_age').sum('Avg_Salary').fillna(0).show()\n",
    "df.groupBy('Customer_main_type').pivot('label').sum('Avg_Salary').fillna(0).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9202bb2d",
   "metadata": {},
   "source": [
    "## Window Functions or Windowed Aggregates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f579bc73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import col,row_number\n",
    "\n",
    "win = Window.orderBy(df['Avg_Salary'].desc())\n",
    "df=df.withColumn('rank', row_number().over(win).alias('rank'))\n",
    "df.show()\n",
    "\n",
    "#One common requirement is to find the top-three values from a category.\n",
    "#In the following example, we define the window and partition by the Customer \n",
    "#subtype column. Basically, what it does is sort the Avg Salary for each \n",
    "#of the Customer subtype category, so now we can use filter to fetch the \n",
    "#top-three salary values for each group\n",
    "win_1=Window.partitionBy(\"Customer_subtype\").orderBy(df['Avg_Salary'].desc())\n",
    "df=df.withColumn('rank', row_number().over(win_1).alias('rank'))\n",
    "df.groupBy('rank').count().orderBy('rank').show()\n",
    "df.filter(col('rank') < 4).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c0cb06",
   "metadata": {},
   "source": [
    "## Crosstab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d59e57c",
   "metadata": {},
   "outputs": [],
   "source": [
    "surveydf = \n",
    "+------+---------+\n",
    "|Gender| Vote|\n",
    "+------+---------+\n",
    "| Male| Yes|\n",
    "| Male| Yes|\n",
    "| Male| No|\n",
    "| Male|DoNotKnow|\n",
    "| Male| Yes|\n",
    "\n",
    "surveyDf.crosstab(\"Gender\",\"Vote\").show()\n",
    "+-----------+---------+--+---+\n",
    "|Gender_Vote|DoNotKnow|No|Yes|\n",
    "+-----------+---------+--+---+\n",
    "| Male| 2| 5| 5|\n",
    "| Female| 1| 1| 6|\n",
    "+-----------+---------+--+---+"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2ed935",
   "metadata": {},
   "source": [
    "## Repartitioning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba983eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.repartition(10)\\ #df with 10 partitions\n",
    "      .rdd \\\n",
    "      .getNumPartitions()\n",
    "df.coalesce(1).rdd.getNumPartitions() #df with 1 partition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c83b92",
   "metadata": {},
   "source": [
    "## Running Queries Programmatically "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27984cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Temp View from a DataFrame - to start using Spark SQL in memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af72534",
   "metadata": {},
   "source": [
    "#### Registering DataFrames as Views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b40d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "peopledf.createGlobalTempView(\"people\") # #gera uma view que permacenerá em disco após o fim da sessão spark\n",
    "df.createTempView(\"customer\") # Gera uma nova view\n",
    "df.createOrReplaceTempView(\"customer\") #gera e sobrescreve uma view de mesmo nome\n",
    "\n",
    "sqlDF = spark.sql(\"SELECT * FROM people\")\n",
    "\n",
    "\n",
    "# EX\n",
    "parquetFile = spark.read.parquet(\"people.parquet\")\n",
    "parquetFile.createOrReplaceTempView(\"parquetFile\")\n",
    "teenagers = spark.sql(\"SELECT name FROM parquetFile WHERE age >= 13 AND age <= 19\")\n",
    "teenagers.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe175f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify tables\n",
    "spark.tableNames()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee54bc5",
   "metadata": {},
   "source": [
    "##### Query Views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d92cd30",
   "metadata": {},
   "outputs": [],
   "source": [
    "df5 = spark.sql(\"SELECT * FROM customer\").show()\n",
    "peopledf2 = spark.sql(\"SELECT * FROM global_temp.people\")\\\n",
    "               .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2050c61",
   "metadata": {},
   "source": [
    "##### Metadata Refreshing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc740bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.catalog.refreshTable(\"my_table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ccef7a1",
   "metadata": {},
   "source": [
    "## Inspect Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2aba0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes #Return df column names and data types\n",
    "df.show() #Display the content of df\n",
    "df.head() #Return first n rows\n",
    "df.first() #Return first row\n",
    "df.take(2) #Return the first n rows df.schema Return the schema of df\n",
    "df.describe().show() #Compute summary statistics df.columns Return the columns of df\n",
    "df.count() #Count the number of rows in df\n",
    "df.distinct().count() #Count the number of distinct rows in df\n",
    "df.printSchema() #Print the schema of df\n",
    "df.explain() #Print the (logical and physical) plans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "887c45a4",
   "metadata": {},
   "source": [
    "## Output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb868e52",
   "metadata": {},
   "source": [
    "##### Data Structures \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6916615f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd1 = df.rdd #Convert df into an RDD\n",
    "df.toJSON().first() #Convert df into a RDD of string\n",
    "df.toPandas() #Return the contents of df as Pandas DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec95e73e",
   "metadata": {},
   "source": [
    "##### Write & Save to Files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3f98fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(\"firstName\", \"city\")\\\n",
    "       .write \\\n",
    "       .save(\"nameAndCity.parquet\")\n",
    "df.select(\"firstName\", \"age\") \\\n",
    "       .write \\\n",
    "       .save(\"namesAndAges.json\",format=\"json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629ece09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to parquet\n",
    "df_csv = spark.read.csv('singlelargefile.csv')\n",
    "df_csv.write.parquet('data.parquet')\n",
    "df_csv.write.parquet(path='parqData')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ac49f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to csv\n",
    "df.write.csv(path='csvFileDir', header=True,sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f62094",
   "metadata": {},
   "outputs": [],
   "source": [
    "# csv até uma tabela no Sand\n",
    "BASE_VQG_HORA = pd.read_csv('SEU_CSV.csv', sep=';', encoding='UTF-8')\n",
    "BASE_VQG_HORA = spark.createDataFrame(BASE_VQG_HORA) #converte o df_Pandas para um df_Spark\n",
    "BASE_VQG_HORA.createOrReplaceTempView('BASE_VQG_HORA')\n",
    "BASE_VQG_HORA.show()\n",
    "\n",
    "NOME_DATA_FRAME.select('Coluna1', 'Coluna2').write.saveAsTable('sand_intelig_atendimento.nome_tabela_lake')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc39bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#AZURE - Gravar em uma nova Tabela SQL\n",
    "server_name = \"jdbc:sqlserver://{SERVER_ADDR}\"\n",
    "database_name = \"database_name\"\n",
    "url = server_name + \";\" + \"databaseName=\" + database_name + \";\"\n",
    "\n",
    "table_name = \"table_name\"\n",
    "username = \"username\"\n",
    "password = \"password123!#\" # Please specify password here\n",
    "\n",
    "try:\n",
    "  df.write \\\n",
    "    .format(\"com.microsoft.sqlserver.jdbc.spark\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"url\", url) \\\n",
    "    .option(\"dbtable\", table_name) \\\n",
    "    .option(\"user\", username) \\\n",
    "    .option(\"password\", password) \\\n",
    "    .save()\n",
    "except ValueError as error :\n",
    "    print(\"Connector write failed\", error)\n",
    "    \n",
    "#Acrescentar à Tabela SQL\n",
    "try:\n",
    "  df.write \\\n",
    "    .format(\"com.microsoft.sqlserver.jdbc.spark\") \\\n",
    "    .mode(\"append\") \\\n",
    "    .option(\"url\", url) \\\n",
    "    .option(\"dbtable\", table_name) \\\n",
    "    .option(\"user\", username) \\\n",
    "    .option(\"password\", password) \\\n",
    "    .save()\n",
    "except ValueError as error :\n",
    "    print(\"Connector write failed\", error)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d4d4482",
   "metadata": {},
   "source": [
    "# Stopping SparkSession \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64452bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c90d2d",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed73c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis Calculations (UDF)\n",
    "def getAvgSale(saleslist):\n",
    "  totalsales = 0\n",
    "  count = 0\n",
    "  for sale in saleslist:\n",
    "\t\ttotalsales += sale[2] + sale[3]\n",
    "    count += 2\n",
    "  return totalsales / count\n",
    "\n",
    "udfGetAvgSale = udf(getAvgSale, DoubleType())\n",
    "df = df.withColumn('avg_sale', udfGetAvgSale(df.sales_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3894d64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis Calculations (Inline)\n",
    "df = df.read.csv('datafile')\n",
    "df = df.withColumn('avg', (df.total_sales / df.sales_count))\n",
    "df = df.withColumn('sq_ft', df.width * df.length)\n",
    "df = df.withColumn('total_avg_size', udfComputeTotal(df.entries) / df.numEntries)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4bd5dc8",
   "metadata": {},
   "source": [
    "### Spark2Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee248e7",
   "metadata": {},
   "outputs": [],
   "source": [
    ".toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02e7cb5",
   "metadata": {},
   "source": [
    "### Pandas2Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebcac9e1",
   "metadata": {},
   "outputs": [],
   "source": [
    ".createDataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3128d0c2",
   "metadata": {},
   "source": [
    "### SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241633b6",
   "metadata": {},
   "outputs": [],
   "source": [
    ".sql()\n",
    "\n",
    "#For example, a SQL query (using the .sql() method) that references your DataFrame \n",
    "#will throw an error. To access the data in this way, you have to save it as a \n",
    "#temporary table.\n",
    ".createTempView() \n",
    "\n",
    ".createOrReplaceTempView()\n",
    "\n",
    "# Usando Expressoes SQL\n",
    "df = df.withColumn(\"comprar\", F.expr(\"CASE WHEN Ano < 2020 THEN 'sim' ELSE 'nao' END\"))1 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9513e66c",
   "metadata": {},
   "source": [
    "## User-Defined Functions (UDFs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dde40ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "df.groupby(\"Avg_age\").count().show()\n",
    "\n",
    "# Adding a Column of Categorical Variables with Labels\n",
    "def age_category(age):\n",
    "    if age == \"20-30 years\":\n",
    "        return \"Young\"\n",
    "    elif age== \"30-40 years\":\n",
    "        return \"Mid Aged\"\n",
    "    elif ((age== \"40-50 years\") or (age== \"50-60 years\")) :\n",
    "        return \"Old\"\n",
    "    else:\n",
    "        return \"Very Old\"\n",
    "\n",
    "age_udf=udf(age_category,StringType())\n",
    "df=df.withColumn('age_category',age_udf(df['Avg_age']))\n",
    "df.select('Avg_age','age_category').show()\n",
    "df.groupby(\"age_category\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b169d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# udf para formatar as strings\n",
    "udf_capitalize = F.udf(lambda x: str(x).capitalize(), StringType())\n",
    "base = base.withColumn(\"dialog\", udf_capitalize(\"dialog\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7973323a",
   "metadata": {},
   "source": [
    "## Pandas UDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8576521c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    "\n",
    "min_sal=1361\n",
    "max_sal=48919896   \n",
    "\n",
    "def scaled_salary(salary):\n",
    "    scaled_sal=(salary-min_sal)/(max_sal-min_sal)\n",
    "    return scaled_sal\n",
    "\n",
    "scaling_udf = pandas_udf(scaled_salary, DoubleType())\n",
    "df.withColumn(\"scaled_salary\",scaling_udf(df['Avg_Salary'])).show(10,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d98e05e0",
   "metadata": {},
   "source": [
    "## Koalas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65aa7c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "import databricks.koalas as pd\n",
    "\n",
    "#a biblioteca koalas contém a maioria das funções pandas, \n",
    "#então basta substituir a lib e continuar usando os codigos em pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e31f1ed1",
   "metadata": {},
   "source": [
    "# DIVERSOS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "985eea37",
   "metadata": {},
   "source": [
    "### UNION - Coalesce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f453d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Problem:\n",
    "+----+-----+-----+-----+-----+-----+---+------+------+------+\n",
    "| ID | in1 | in2 | in3 | in4 | in5 | / | out1 | out2 | out3 |\n",
    "+----+-----+-----+-----+-----+-----+---+------+------+------+\n",
    "|  1 |     |     | C   |     |     | / | C    |      |      |\n",
    "|  2 | A   |     | C   |     | E   | / | A    | C    | E    |\n",
    "|  3 | A   | B   | C   |     |     | / | A    | B    | C    |\n",
    "|  4 | A   | B   | C   | D   | E   | / | A    | B    | C    |\n",
    "|  5 |     |     |     |     |     | / |      |      |      |\n",
    "|  6 |     | B   |     |     | E   | / | B    | E    |      |\n",
    "|  7 |     | B   |     | D   | E   | / | B    | D    | E    |\n",
    "+----+-----+-----+-----+-----+-----+---+------+------+------+\n",
    "\n",
    "Solution:\n",
    "    \n",
    "import pyspark.sql.functions as f\n",
    "\n",
    "df = spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").csv(\"test.csv\")\n",
    "\n",
    "cols = df.columns\n",
    "cols.remove('ID')\n",
    "\n",
    "df2 = df.withColumn('ins', f.array_except(f.array(*cols), f.array(f.lit(None))))\n",
    "\n",
    "for i in range(0, 3):\n",
    "    df2 = df2.withColumn('out' + str(i+1), f.col('ins')[i])\n",
    "    \n",
    "df2.show(10, False)\n",
    "\n",
    "+---+----+----+----+----+----+---------------+----+----+----+\n",
    "|ID |in1 |in2 |in3 |in4 |in5 |ins            |out1|out2|out3|\n",
    "+---+----+----+----+----+----+---------------+----+----+----+\n",
    "|1  |null|null|C   |null|null|[C]            |C   |null|null|\n",
    "|2  |A   |null|C   |null|E   |[A, C, E]      |A   |C   |E   |\n",
    "|3  |A   |B   |C   |null|null|[A, B, C]      |A   |B   |C   |\n",
    "|4  |A   |B   |C   |D   |E   |[A, B, C, D, E]|A   |B   |C   |\n",
    "|5  |null|null|null|null|null|[]             |null|null|null|\n",
    "|6  |null|B   |null|null|E   |[B, E]         |B   |E   |null|\n",
    "|7  |null|B   |null|D   |E   |[B, D, E]      |B   |D   |E   |\n",
    "+---+----+----+----+----+----+---------------+----+----+----+"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc4d412",
   "metadata": {},
   "source": [
    "#### Calcular o Tempo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04985670",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "\n",
    "def calcular_tempo(funcao):\n",
    "    def wrapper():\n",
    "        tempo_inicial = time.time()\n",
    "        funcao()\n",
    "        tempo_final = time.time()\n",
    "        print(f\"Dutação foi de {tempo_final-tempo_inicial} segundos\")\n",
    "    return wrapper\n",
    "\n",
    "@calcular_tempo\n",
    "def pegar_cotacao_dolar():\n",
    "    link = f\"https://economia.awesomeapi.com.br/last/USD-BRL\"\n",
    "    requisicao = requests.get(link)\n",
    "    requisicao = requisicao.json()\n",
    "    print(requisicao['USDBRL']['bid'])\n",
    "\n",
    "pegar_cotacao_dolar()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
